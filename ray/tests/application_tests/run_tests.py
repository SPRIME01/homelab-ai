#!/usr/bin/env python3
"""
Main test runner for Ray application tests.
"""

import os
import sys
import time
import json
import yaml
import logging
import argparse
import unittest
from typing import Dict, List, Optional

from .base_test import RayBaseTest
from .task_distribution_test import TaskDistributionTest
from .resource_allocation_test import ResourceAllocationTest
from .fault_tolerance_test import FaultToleranceTest
from .service_integration_test import ServiceIntegrationTest
from .ai_workload_test import AIWorkloadTest

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("ray_test_runner")

def generate_html_report(results: Dict, output_dir: str) -> None:
    """Generate HTML report from test results."""
    report_path = os.path.join(output_dir, "ray_test_report.html")

    with open(report_path, "w") as f:
        f.write("""<!DOCTYPE html>
<html>
<head>
    <title>Ray Application Test Results</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1, h2, h3 { color: #333; }
        .summary { background-color: #f5f5f5; padding: 15px; border-radius: 5px; }
        .passed { color: green; }
        .failed { color: red; }
        table { border-collapse: collapse; width: 100%; margin-top: 20px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
    </style>
</head>
<body>
    <h1>Ray Application Test Results</h1>
    <div class="summary">
        <p><strong>Timestamp:</strong> """ + time.strftime('%Y-%m-%d %H:%M:%S') + """</p>
        <p><strong>Total Tests:</strong> """ + str(results["total_tests"]) + """</p>
        <p><strong>Passed:</strong> <span class="passed">""" + str(results["passed"]) + """</span></p>
        <p><strong>Failed:</strong> <span class="failed">""" + str(results["failed"]) + """</span></p>
    </div>

    <h2>Test Results</h2>
    <table>
        <tr>
            <th>Test Case</th>
            <th>Status</th>
            <th>Duration</th>
            <th>Details</th>
        </tr>
""")

        # Add rows for each test result
        for test_name, test_data in results["tests"].items():
            status = "PASSED" if test_data["success"] else "FAILED"
            status_class = "passed" if test_data["success"] else "failed"

            f.write(f"""
        <tr>
            <td>{test_name}</td>
            <td class="{status_class}">{status}</td>
            <td>{test_data["duration"]:.2f}s</td>
            <td>{test_data.get("details", "")}</td>
        </tr>""")

        # Add performance results if available
        if "performance" in results:
            f.write("""
    </table>

    <h2>Performance Results</h2>
    <table>
        <tr>
            <th>Metric</th>
            <th>Value</th>
        </tr>
""")

            for metric, value in results["performance"].items():
                f.write(f"""
        <tr>
            <td>{metric}</td>
            <td>{value}</td>
        </tr>""")

        f.write("""
    </table>

    <footer>
        <p>Generated by Ray Application Testing Framework</p>
    </footer>
</body>
</html>
""")

    logger.info(f"HTML report generated at {report_path}")

def main():
    """Run the test suite."""
    parser = argparse.ArgumentParser(description="Ray Application Test Runner")

    parser.add_argument("--tests", nargs="+",
                      help="Specific test classes to run (default: all)")
    parser.add_argument("--ray-address", default="auto",
                      help="Ray cluster address")
    parser.add_argument("--namespace", default="default",
                      help="Ray namespace")
    parser.add_argument("--output-dir", default="test_results",
                      help="Directory for test outputs")
    parser.add_argument("--config-file", help="Path to config YAML file")
    parser.add_argument("--html-report", action="store_true",
                      help="Generate HTML report")
    parser.add_argument("--junit-xml", help="Generate JUnit XML report")

    args = parser.parse_args()

    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)

    # Available test classes
    test_classes = {
        "task": TaskDistributionTest,
        "resource": ResourceAllocationTest,
        "fault": FaultToleranceTest,
        "service": ServiceIntegrationTest,
        "ai": AIWorkloadTest
    }

    # Filter test classes if specified
    if args.tests:
        filtered_classes = {}
        for test_name in args.tests:
            if test_name in test_classes:
                filtered_classes[test_name] = test_classes[test_name]
            else:
                logger.warning(f"Unknown test class: {test_name}")
        test_classes = filtered_classes

    if not test_classes:
        logger.error("No test classes to run")
        sys.exit(1)

    # Create test suite
    suite = unittest.TestSuite()

    # Add test classes to suite
    for name, test_class in test_classes.items():
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        suite.addTests(tests)

    # Run tests with appropriate runner
    if args.junit_xml:
        try:
            import xmlrunner
            runner = xmlrunner.XMLTestRunner(
                output=os.path.join(args.output_dir, "junit-xml"),
                verbosity=2
            )
        except ImportError:
            logger.warning("xmlrunner not installed. Using standard runner.")
            runner = unittest.TextTestRunner(verbosity=2)
    else:
        runner = unittest.TextTestRunner(verbosity=2)

    # Run tests
    result = runner.run(suite)

    # Generate summary
    summary = {
        "timestamp": time.time(),
        "total_tests": result.testsRun,
        "passed": result.testsRun - len(result.failures) - len(result.errors),
        "failed": len(result.failures) + len(result.errors),
        "skipped": len(getattr(result, 'skipped', [])),
        "tests": {}
    }

    # Add detailed test results
    for test_class in test_classes.values():
        if hasattr(test_class, 'test_results'):
            summary["tests"].update(test_class.test_results.get("tests", {}))

            # Add performance metrics if available
            if "performance" in test_class.test_results:
                if "performance" not in summary:
                    summary["performance"] = {}
                summary["performance"].update(test_class.test_results["performance"])

    # Save summary to JSON
    summary_path = os.path.join(args.output_dir, "test_summary.json")
    with open(summary_path, "w") as f:
        json.dump(summary, f, indent=2)

    # Generate HTML report if requested
    if args.html_report:
        generate_html_report(summary, args.output_dir)

    # Set exit code based on test results
    sys.exit(0 if result.wasSuccessful() else 1)

if __name__ == "__main__":
    main()
